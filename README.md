# GugaPPT - Gustavo's PowerPoint Generator

An application that generates PowerPoint presentations from a topic using an MCP (Model Context Protocol) server, multiple AI agents, and web research with source validation.

On the **Home page** (`http://localhost:8000/`), you can use a simple web interface to generate presentations interactively by entering a topic and choosing the number of slides. Once submitted, the generated PowerPoint file can be downloaded directly from the same page when ready.

---

> **Note:** Example presentations generated by this system can be found in the `concluded_presentations/` directory. Each run stores its `.pptx` and any chart images in this folder for review.

---

## Folder Structure

```
aristotle-challenge/
  .env.example              # Example environment variables (TAVILY_API_KEY, OPENAI_API_KEY)
  .gitignore
  docker-compose.yml       # Compose file to run the backend in Docker
  README.md
  concluded_presentations/ # Output directory for generated .pptx files and chart images (see examples here)
    charts/                # PNG charts generated by the illustrator

  src-backend/             # Backend application (Python/FastAPI)
    main.py                # FastAPI app entry point; mounts presentation router and home page
    pyproject.toml         # Project and dependency definition (uv)
    uv.lock                # Locked dependencies
    Dockerfile             # Multi-stage image for running the API in Docker

    app/                   # Web layer
      routes/
        presentation/      # Presentation API (generate, download, status via SSE)
          router.py       # Endpoints: POST /presentation/generate_ppt, GET /download/{pprt_id}, GET /status/{pprt_id}
          schemas.py      # Request/response Pydantic models
          utils.py        # Helpers (e.g. presentation ID generation)
      templates/
        home.html         # Front-end page to trigger generation and download

    core/                  # Shared configuration and constants
      consts.py           # Paths, fonts, slide dimensions, DOMAIN_BLACKLIST for search
      logger_config.py    # Logging setup
      settings.py        # Pydantic settings (API keys loaded from .env)

    mcp_server/           # MCP server and orchestration
      mcp_server.py       # FastMCP server; defines tools: search_web, create_presentation, generate_chart
      workflow.py        # run_ppt_workflow: orchestrates Planner -> Researcher -> Writer -> Illustrator -> create_presentation

      agents/             # LLM-based agents (OpenAI)
        planner/          # Builds presentation outline (slide titles + search queries)
        researcher/       # Calls search_web and summarizes facts per slide
        writer/           # Drafts slide content, speaker notes, sources, and visual requests
        illustrator/      # Calls generate_chart for requested visuals

      helper/
        ppt_style.py      # Styling for title and body placeholders in PPTX
        source_validator.py  # URL validation, scoring, and tier ranking for search results

    tests/
      test_workflow.py    # Tests for the presentation workflow
```

---

## Agents

The pipeline is driven by four agents that run in sequence. Each uses the MCP session to call tools when needed.

1. **Planner** (`mcp_server/agents/planner/`)
   - **Role:** Produces a presentation outline from the topic and requested number of slides.
   - **Output:** A plan with one entry per slide: title and a list of search queries for that slide.
   - **Model:** GPT-4o-mini. Response is parsed into a structured `PresentationPlan` (slides with titles and search_queries).
   - **Validation:** Retries up to 3 times if the model returns nothing; checks that the number of slides matches the request.

2. **Researcher** (`mcp_server/agents/researcher/`)
   - **Role:** For each slide in the plan, runs the `search_web` tool with that slide’s queries and turns raw results into concise facts.
   - **Output:** A `ResearchSummary` per slide (slide_topic, facts).
   - **Model:** GPT-4o-mini. Summaries are parsed into structured form.
   - **Validation:** Retries up to 3 times if the model returns nothing.

3. **Writer** (`mcp_server/agents/writer/`)
   - **Role:** Combines the plan and research into full slide content: titles, bullet points, speaker notes, source URLs, and visual requests (e.g. chart type and data).
   - **Output:** `PresentationContent` (slides with title, points, speaker_notes, sources, optional visual_request).
   - **Model:** GPT-4o. Ensures at least one slide has a chart request; retries up to 3 times otherwise.
   - **Validation:** Retries on null response or when no chart is requested.

4. **Illustrator** (`mcp_server/agents/illustrator/`)
   - **Role:** For each slide that has a `visual_request` of type chart, calls the `generate_chart` tool with the requested data and title.
   - **Output:** `IllustrationResult` with a list of assets (slide_number, file_path, etc.).
   - **Model:** GPT-4o-mini (used only if needed for interpreting requests; chart creation is done by the tool).
   - **Validation:** Failures for a single visual are logged; the workflow continues with the rest.

---

## Tools (MCP)

The MCP server (`mcp_server/mcp_server.py`) exposes three tools used by the workflow.

| Tool | Description |
|------|-------------|
| **search_web** | Uses Tavily to search the web with a query and optional `search_depth` ("basic" or "advanced"). Results are passed through the source validator; only sources in tier S or A are returned as JSON. Lower-tier or invalid sources are dropped. |
| **create_presentation** | Accepts a filename and a JSON payload describing slides (title, points, optional image path, speaker_notes, sources). Builds a PowerPoint with the configured layout and styles, adds speaker notes and source URLs, and saves the file under `concluded_presentations/`. |
| **generate_chart** | Accepts `data_json` (labels and values), `chart_type` ("bar", "pie", or "line"), and `title`. Renders the chart with matplotlib, saves it under `concluded_presentations/charts/`, and returns the image path for the writer to pass into `create_presentation`. |

---

## Source Validation Logic

Source validation is implemented in `mcp_server/helper/source_validator.py`. It is used inside the **search_web** tool so that only higher-quality results are returned to the Researcher.

**Flow:**

1. **Normalize URL**  
   Query parameters (e.g. UTM) and fragments are stripped so the same page is not scored multiple times.

2. **Health check**  
   A GET request is sent to the normalized URL (with a browser-like User-Agent and a 5s timeout). Non-200 responses mark the source as "dead" and it is not used.

3. **Metadata extraction**  
   From the HTML (BeautifulSoup):
   - **Author:** `<meta name="author">` or `<meta property="article:author">`
   - **Date:** `<meta name="date">`, `article:published_time`, or `<time>`
   - **References:** Whether any heading (h1–h4) contains words like "references", "bibliography", "works cited", "sources"

4. **Scoring**  
   - Base: Tavily confidence for that result, scaled to 0–100.
   - Author present: +10.
   - Date present: +5.
   - Domain ends with `.edu` or `.gov`: +15.
   - Final score is capped at 100.

5. **Tier**  
   - Score >= 80: **S**  
   - Score >= 60: **A**  
   - Otherwise: **B** (and below S/A).

6. **Ranking and filtering**  
   `rank_sources` runs the above for each raw result and sorts by final score descending. In **search_web**, only results with tier **S** or **A** are returned; B and below are discarded. If no S/A result exists for the query, the tool returns an empty list.

**Domain blacklist:**  
Search results from domains in `core/consts.py` (e.g. reddit.com, quora.com, twitter.com, youtube.com) are excluded at the Tavily call level and never reach the validator.

---

## Docker and Running the Application

### Prerequisites

- Docker and Docker Compose.
- A `.env` file at the project root with:
  - `TAVILY_API_KEY` – for web search.
  - `OPENAI_API_KEY` – for the agents.

Copy from `.env.example` and fill in the values.

### Run with Docker Compose

From the repository root (where `docker-compose.yml` is):

```bash
docker compose up --build
```

- The **Dockerfile** lives in `src-backend/`.

- **docker-compose.yml** builds that image from `./src-backend`, maps port **8000**, loads `.env`, and mounts `./concluded_presentations` so generated presentations and charts persist on the host. A healthcheck hits `http://localhost:8000/docs` periodically.

Once the container is up:

- **API docs:** http://localhost:8000/docs  
- **Home page:** http://localhost:8000/  
- **Generate a presentation:** POST to `/presentation/generate_ppt` with JSON body `{"topic": "...", "slides": N}`. Use the returned `pprt_id` to poll `/presentation/status/{pprt_id}` (SSE) or download via `/presentation/download/{pprt_id}` when ready.

### Run locally (without Docker)

From `src-backend/`:

```bash
uv sync
uv run uvicorn main:api --reload --host 0.0.0.0 --port 8000
```

Use the same `.env` at the project root (or ensure `core.settings` can load your env file). Generated files will appear in `concluded_presentations/` relative to the project root.

### Run tests

**With Docker Compose** (from the repository root):

```bash
docker compose run --rm backend uv run pytest
```

**With uv** (from `src-backend/`):

```bash
uv run pytest
```
